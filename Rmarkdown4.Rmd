---
title: "IT Service Dashboard"
output: html_notebook
---

# Background

It is IT incident data downloaded in the Servicenow, SNOW between 2018-09-01 to 2019-11-30, last refreshed at 2019-12-01.
Resolution lead time is the dependent variable and is going to analyze the independent data what main factors delay the time, 
and develop the model to predict the potential long-time resolution incident 

## Data exploration

The data consist of `r n1` rows and `r c1` columns.
Let's review the sample data

```{r, echo=FALSE}
knitr::kable(
  data[1:5,1:7], caption ="IT incident data"
)
```

There are many data created by the system automatically, it is likely to be caused by some background automatic schedule.
This will make the analysis more complicate, thus in this analysis we will delete the data not created by the users

```{r 00110, echo=FALSE}
cat("Incident data is creatd by midserver or guest:", comma(n1-n2,digits=0), 
    "out of", comma(n1,digits=0), ",", round((n1-n2)/n1*100), "% deleted from the data")
```

* Resolution Lead Time (LT_day)
The first thing is to check the objective value which is resolution lead time.
Let's see the density graph with the variable of Resolution Lead Time(RLT)
and many incidents can be observed over 10, 20 even 40 RLT days.     

* LT_day is converted feature from business_duration whose UOM is a second excluding weekend and pending time, this requires to convert to a business work day
```{r 00115, echo=FALSE}
data2 %>% filter(!is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT") %>% ggplot() + geom_density(aes(x=LT_day, ..scaled..)) + scale_x_log10(breaks = c(0, 1, 2, 3,5, 10,20, 40, 60))

```

According to RLT distribution, 90% of data is within 5 days lead time, 95% within 20 days, and 99% within 60 days.
And over 99% of data has more than 90 days, it seems to be outlier, or abandoned request 

```{r 00116, echo=FALSE}
quantile( filter(data, !is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT")$LT_day, probs = c(0.5, 0.75, 0.8, 0.9, 0.93, 0.95,0.96, 0.97, 0.98, 0.99 , 0.996 ,1))
```

To check RLT value with other features, tickets reassigned more than 3 times or reopened has high density around 8 days. 
And FTR show high density on 1, 2 days RLT.
Later it will validate how much they are corelated to RLT 
* Question, in which area is FTR high and why?

```{r 00117, echo=FALSE}
s1 <-
data %>% filter(reassign == 1) %>% filter(!is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT") %>% 
  ggplot() + geom_density(aes(x=LT_day, ..scaled..)) + scale_x_log10(breaks = c(0, 1, 2, 3,5, 10,20, 40, 60)) + 
  ggtitle("RLT distribution with features", subtitle = "Reassigned >=3")

s2 <-
data %>% filter(reopen == 1) %>% filter(!is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT") %>% 
  ggplot() + geom_density(aes(x=LT_day, ..scaled..)) + scale_x_log10(breaks = c(0, 1, 2, 3,5, 10,20, 40, 60)) +
  ggtitle("",subtitle = "Reopened")
  
s3 <-
data %>% filter(u_FTR2 == 1) %>% filter(!is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT") %>% 
  ggplot() + geom_density(aes(x=LT_day, ..scaled..)) + scale_x_log10(breaks = c(0, 1, 2, 3,5, 10,20, 40, 60)) + 
  ggtitle("",subtitle = "FTR supported")

grid.arrange(s1, s2, s3, widths=1)
```

Here. there are some abnormal findings that RLT is quite short but duration in the calendar is very long.
This account for 3% of the data, and need to check the gap why it occurs.
Let's see the trend of the gap between RLT and calendar period.
In the distribution around 8~18 days gap is considerably high. 

```{r 00118, echo=FALSE}
data %>% filter(abs(LT_day-weekdays) > 5) %>%
  ggplot() + geom_density(aes(x=LT_day-weekdays)) + scale_x_log10(breaks = c(0, 1, 2, 3,5, 10,20, 40, 60, 300))
```

For example, #INC1854118 has 3 hours RLT but actual number of days from the opening to reslove in the calendar is 71 days.
It needs to indentify the root cause of this result.

```{r 00119, echo=FALSE}
data %>% filter(number %in% c("INC1854118", "INC2029235", "INC2019483")) %>% select(number, created_on2, resolved2, business_duration, weekdays)
```

Another interesting finding is that if business_duration is equal to calendar_duration, SNOW calculates a day with 24 hours, not eight work hours, and also include the weekend.
This error is obvious when calendar_duration has a high number, i.e.,>= 20 hours (72000)
This need to check why it has big gap to calculate the lead time in what kind of condition since this may cause disagreement between business and IT

```{r 00120, echo=FALSE}
data %>% mutate(NG = ifelse(business_duration == calendar_duration & calendar_duration >= 72000 & !is.na(resolved), 1,0), 
                yyyymm = as.Date(str_c(substr(created_on, 1,7), "-01"))) %>% 
  group_by(yyyymm) %>% dplyr::summarise(wrong_duration = sum(NG), total = n(), percent = comma(wrong_duration/total*100)) %>% arrange(desc(yyyymm))

```

## Missing value handling {.tabset}
Check missing values in the data

```{r 00130, echo=FALSE}
NAcol <- which(colSums(is.na(data))>0)
NAcols <- sort(colSums(sapply(data[NAcol],is.na)), decreasing = T)
NAcols
```

### Locationname
**country missing value is reduced to 10,869, this is due to no location info**

```{r, include=FALSE}
#data %>% filter(is.na(Y_country)) %>% distinct(Y_locationname) %>% write_csv("country_missing.csv")
#data %>% distinct(Y_country) %>% write_csv("country.csv")
country_location <- read_csv("./data/Country_location_master.csv")
```

```{r, include=FALSE}
data <-
data %>% left_join(country_location, by = c("Y_locationname" = "Location")) %>%
  mutate(Y_country = ifelse(is.na(Country),Y_country,Country))
data$Country <- NULL
```

after imputation, missing country data is reduced 62%, (28707 -> 10974)

### Region
**Fill the missing region data with the most frequent value**

```{r, include=FALSE}
region <-
  data %>% group_by(Y_country, Y_timezone) %>% dplyr::summarise(n = n()) 

region <-
region %>% group_by(Y_country) %>% top_n(1, n)

region <-
region %>% mutate(Y_timezone = ifelse(Y_country %in% c("United States of America", "Canada"),"NAm",
                                      ifelse(Y_country %in% c("Kenya", "Nigeria", "Russian Federation"),"EMEA", 
                                             ifelse(Y_country %in% c("Pakistan", "Viet Nam"),"AP", 
                                                    ifelse(Y_country %in% c("Venezuela (Bolivarian Republic of)"),"LA", Y_timezone))))) %>%
  filter(!is.na(Y_timezone))

data <-
  data %>% left_join(region, by = "Y_country") 

data$Y_timezone.x <- NULL
data$n <- NULL
data <- data %>% dplyr::rename( Y_timezone = Y_timezone.y)
```

after imputation, missing region data is reduced 93%, (151592 -> 10974)

### Jobfamily
**Fill the missing jobfamily with the most frequent value of userid**

```{r, include=FALSE}
data %>% group_by(Z_jobfamily) %>% distinct(Z_jobfamily)
data %>% group_by(Z_position) %>% distinct(Z_position)

jobfamily <- 
  data %>% group_by(Z_cmguserid, Z_jobfamily) %>% dplyr::summarise(n = n()) %>% write_csv("job.csv")

jobfamily <-
  jobfamily %>% group_by(Z_cmguserid) %>% top_n(1, n) %>% filter(!is.na(Z_jobfamily))

data <-
  data %>% left_join(jobfamily, by = "Z_cmguserid") %>%
  mutate(Z_jobfamily.x = ifelse(is.na(Z_jobfamily.x), Z_jobfamily.y, Z_jobfamily.x))
data$Z_jobfamily.y <- NULL
data$n <- NULL
data <- data %>% dplyr::rename( Z_jobfamily = Z_jobfamily.x)

```

**Again let's fill the missing jobfamily with the most frequent value of position**
```{r, include=FALSE}
position <- 
  data %>% group_by(Z_position, Z_jobfamily) %>% dplyr::summarise(n = n())

position <-
  position %>% group_by(Z_position) %>% top_n(1, n) %>% filter(!is.na(Z_jobfamily))

data <-
  data %>% left_join(position, by = "Z_position") %>%
  mutate(Z_jobfamily.x = ifelse(is.na(Z_jobfamily.x), Z_jobfamily.y, Z_jobfamily.x))
data$Z_jobfamily.y <- NULL
data$n <- NULL
data <- data %>% dplyr::rename( Z_jobfamily = Z_jobfamily.x)
```

after imputation, missing jobfamily data is reduced 2%, (119922 -> 118218)

```{r, echo=FALSE}
NAcol <- which(colSums(is.na(data))>0)
NAcols2 <- sort(colSums(sapply(data[NAcol],is.na)), decreasing = T)
NAcols2
```

##Missing value only for internal emplyee 

Assinging missing Country, Region data with the mode of related data is successful, but not significant imputation for job family.
Later incident data will be screened by requestor whose internal employee indicator is "Y" then these missing value will be reduced again.
In this case, total data is 486531, missing data 9559 / thus missing Country, Region, job family is less than 2% for Merck internal requestor.
This is good result after null imputation.

```{r, echo=FALSE}
temp <-
data %>% filter(substr(Z_cmguserid,1,1) == "M")

NAcol <- which(colSums(is.na(temp))>0)
NAcols3 <- sort(colSums(sapply(temp[NAcol],is.na)), decreasing = T)
n3 <- nrow(temp)
NAcols3
```

###BSID
BSID value is important because this is the baseline measurement with business agreement 
**Fill the missing BSID with the most frequent value of assignment group**
```{r, include=FALSE}
#Fill the missing BSID with the most frequent value of assignment_Group
data %>% group_by(X_bs_id) %>% distinct(X_bs_id)
BSID <- 
  data %>% group_by(assignment_grp, X_bs_id) %>% dplyr::summarise(n = n())

BSID <-
  BSID %>% group_by(assignment_grp) %>% top_n(1, n) %>% filter(!is.na(X_bs_id))

data <-
  data %>% left_join(BSID, by = "assignment_grp") %>%
  mutate(X_bs_id.x = ifelse(is.na(X_bs_id.x), X_bs_id.y, X_bs_id.x))
data$X_bs_id.y <- NULL
data$n <- NULL
data <- data %>% dplyr::rename( X_bs_id = X_bs_id.x)
```

* 42,477 out of 83,555 missing BSID is still remained as missing(51%) after null value handling
* with assignment group & service component we can found that most of the missing data is related to SIAL, 34,705 out of 42,777(82%)
* Every momth it occurs around 2500 SIAL incidents which don't have BSID, whereas around 4500 has BSID every month in SIAL ticket
* It needs to identify the causes of missing BSID & improve the situation

```{r, echo=FALSE}
data %>% filter(str_detect(assignment_grp, "SIAL") | str_detect(X_service_component, "SIAL")) %>% filter(!is.na(X_bs_id)) %>% 
  group_by(yyyymm) %>%  dplyr::summarise(n = n())
```

Let's try again with IT engineer who resolve the incident to fill the BSID
38,093 out of 42,477 missing BSID is still remained as missing(89%) after null imputation with IT engineer
Still we have high number of missing BSID, but it is less than 1% of total incidents

```{r, eval=FALSE}
BSID <- 
  data %>% group_by(resolved_by_id, X_bs_id) %>% dplyr::summarise(n = n())

BSID <-
  BSID %>% group_by(resolved_by_id) %>% top_n(1, n) %>% filter(!is.na(resolved_by_id)) %>% filter(!is.na(X_bs_id)) %>% filter(n > 1)

data <-
  data %>% left_join(BSID, by = "resolved_by_id") %>%
  mutate(X_bs_id.x = ifelse(is.na(X_bs_id.x), X_bs_id.y, X_bs_id.x))
data$X_bs_id.y <- NULL
data$n <- NULL
data <- data %>% dplyr::rename( X_bs_id = X_bs_id.x)
```

If it narrow down the incidents created only by the business user not IT, it is 30534 missing values out of total 466,393, 6.5%

```{r, echo=FALSE}
NAcol <- which(colSums(is.na(data))>0)
NAcols5 <- sort(colSums(sapply(data[NAcol],is.na)), decreasing = T)
#NAcols2 #83555 -> 42477 -> 38673 (for internal)

data %>% filter(substr(Z_cmguserid,1,1) == "M", !is.na(resolved), request_div != "IT") %>%  dplyr::summarise(Miss_BSD = sum(is.na(X_bs_id)), Not_Miss_BSD = sum(!is.na(X_bs_id)), 
                                                                                        all_biz_resolved_incidents = Miss_BSD+Not_Miss_BSD, Miss_BSD/(Miss_BSD+Not_Miss_BSD)*100)
```

###Data Review again

Finally let's see characteristics of all the data created by Merck employee including IT

Character data such as "incident_type, request_div, urgency, Y_country, Y_timezone, Z_jobfamily, state, state2" has less than 2% missing data, 
and values(features) in the data are maximum 120. Later these data will be transformed to categorical data

```{r, echo=FALSE}
skimmed %>% filter(type == "character") %>% arrange(type, as.numeric(n_unique), desc(missing))
```

For Numeric data such as LT_day(lead time in day), LT_hour 75th quantile is 2.6 days with 9.5 standard deviation,

```{r, echo=FALSE}
#skimmed %>% filter(type == "numeric") %>% arrange(type, as.numeric(n_unique), desc(missing))
skimmed %>% filter(variable %in% c("LT_day","LT_hour")) %>% select(variable, mean,sd,starts_with("p"))
```

It is true that lead time of most incidents is located within 10 days, but still many data also stay between 15 ~ 45days.

```{r, echo=FALSE}
data %>% filter(substr(Z_cmguserid,1,1) == "M", !is.na(resolved)) %>% 
  ggplot(aes(LT_day)) + geom_histogram(binwidth = 10) + xlim(10,50) 
```

###Reassignment, Reopen

There needs to check the case when Reassignment >= 4, reopen_count >= 2, as it is related to not clear service matirix as well as service lead time