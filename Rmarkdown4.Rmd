---
title: "IT Service Dashboard"
output: html_notebook
---

# Background

It is IT incident data downloaded in the Servicenow, SNOW between 2018-09-01 to 2019-11-30, last refreshed at 2019-12-01.
Resolution lead time is the dependent variable and is going to analyze the independent data what main factors delay the time, 
and develop the model to predict the potential long-time resolution incident 

## Data exploration

The data consist of `r n1` rows and `r ncol(data)` columns.
Let's review the sample data

```{r, echo=FALSE}
knitr::kable(
  data[1:5,1:7], caption ="IT incident data"
)
```

There are many data created by the system, it is likely that this is caused by some background schedule.
This will make the model more complicate, thus in this analysis we will delete the data not created by the users

```{r 00110, echo=FALSE}
cat("Incident data is creatd by midserver or guest:", comma(n1-n2,digits=0), 
    "out of", comma(n1,digits=0), ",", round((n1-n2)/n1*100), "%")
```

The first thing is to check the objective value which is resolution lead time.
Let's see the density graph with the variable of Resolution Lead Time(RLT). 

Most of RLT is within 5 days, but still RLT of many incidents can be seen over 10, 20 even 40 days.     

* LT_day is converted variable from business_duration whose UOM is a second, excluding weekend and pending time
```{r 00115, echo=FALSE}
data %>% filter(!is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT") %>% ggplot() + geom_density(aes(x=LT_day, ..scaled..)) + scale_x_log10(breaks = c(0, 1, 2, 3,5, 10,20, 40, 60))

```

5% of data is over 20 days RLT
```{r 00116, echo=FALSE}
quantile( filter(data, !is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT")$LT_day, probs = c(0.5, 0.75, 0.8, 0.9, 0.93, 0.95,0.96, 0.97, 0.98, 0.99 , 0.996 ,1))
```


#Data distribution review
data %>% filter(!is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT") %>% ggplot() + geom_density(aes(x=LT_day, ..scaled..)) + scale_x_log10(breaks = c(0, 1, 2, 3,5, 10,20, 40, 60))
summary(data$LT_day)
quantile(data$LT_day, probs = c(0.5, 0.75, 0.8, 0.9, 0.93, 0.95, 0.97, 0.98, 0.99 ,1),na.rm = FALSE)

temp <-
data %>% filter(!is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT")
quantile(temp$LT_day, probs = c(0.5, 0.75, 0.8, 0.9, 0.93, 0.95,0.96, 0.97, 0.98, 0.99 ,1))

data %>% filter(reassign == 1) %>% filter(!is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT") %>% ggplot() + geom_density(aes(x=LT_day, ..scaled..)) + scale_x_log10(breaks = c(0, 1, 2, 3,5, 10,20, 40, 60))
data %>% filter(reopen == 1) %>% filter(!is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT") %>% ggplot() + geom_density(aes(x=LT_day, ..scaled..)) + scale_x_log10(breaks = c(0, 1, 2, 3,5, 10,20, 40, 60))
data %>% filter(u_FTR2 == 1) %>% filter(!is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT") %>% ggplot() + geom_density(aes(x=LT_day, ..scaled..)) + scale_x_log10(breaks = c(0, 1, 2, 3,5, 10,20, 40, 60))

data %>% unique(reassignment_cnt)
data %>% filter(!is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT") %>% ggplot() + geom_density(aes(x=reassignment_cnt)) + scale_x_log10(breaks = c(0,1, 2,3,4,5, 10, 20, 30, 40, 60))
data %>% filter(reassign == 1) %>% filter(!is.na(resolved), LT_day >0, !is.na(Y_cmg_shortname) | request_div != "IT") %>% ggplot() + geom_point(aes(reassignment_cnt, LT_day ))




If business_duration is equal to calendar_duration, SNOW calculates a day with 24 hours, not eight work hours, and also include the weekend.
This error is obvious when calendar_duration has a high number, i.e.,>= 60 hours (216000)

```{r 00120, echo=FALSE}
data %>% filter(business_duration == calendar_duration, calendar_duration >= 216000 , !is.na(resolved)) %>% select(number, created_on, resolved, calendar_duration) %>% arrange(desc(created_on))
```

Every month, more than 7% out of created incidents is the case of business_duration == calendar_duration, it needs to consider when lead time is calculated
```{r, echo=FALSE}
data %>% mutate(NG = ifelse(business_duration == calendar_duration, 1,0), 
                yyyymm = as.Date(str_c(substr(created_on, 1,7), "-01"))) %>% 
  group_by(yyyymm) %>% dplyr::summarise(wrong_duration = sum(NG), total = n(), percent = comma(wrong_duration/total*100)) %>% arrange(desc(yyyymm))

```

Let's check the distribution of resolution lead time, most of the data are less than 10 days
But, also many data took more than 30 days to resolve the incidents
```{r, echo=FALSE}
data %>% filter(!is.na(resolved)) %>% ggplot(aes(business_duration/3600/8)) + geom_histogram(binwidth = 1, na.rm = TRUE) + xlim(0, 40)
data %>% filter(!is.na(resolved)) %>% ggplot(aes(business_duration/3600/8)) + geom_histogram(binwidth = 1, na.rm = TRUE) + xlim(30, 60)
```

# Missing value handling
Check missing values in the dataset

```{r 00130, echo=FALSE}
NAcol <- which(colSums(is.na(data))>0)
NAcols <- sort(colSums(sapply(data[NAcol],is.na)), decreasing = T)
NAcols
```

##Missing value imputation {.tabset}

###locationname
**country missing value is reduced to 10,869, this is due to no location info**

```{r, include=FALSE}
#data %>% filter(is.na(Y_country)) %>% distinct(Y_locationname) %>% write_csv("country_missing.csv")
#data %>% distinct(Y_country) %>% write_csv("country.csv")
country_location <- read_csv("./data/Country_location_master.csv")
```

```{r, include=FALSE}
data <-
data %>% left_join(country_location, by = c("Y_locationname" = "Location")) %>%
  mutate(Y_country = ifelse(is.na(Country),Y_country,Country))
data$Country <- NULL
```

after imputation, missing country data is reduced 62%, (28707 -> 10974)

###region
**Fill the missing region data with the most frequent value**

```{r, include=FALSE}
region <-
  data %>% group_by(Y_country, Y_timezone) %>% dplyr::summarise(n = n()) 

region <-
region %>% group_by(Y_country) %>% top_n(1, n)

region <-
region %>% mutate(Y_timezone = ifelse(Y_country %in% c("United States of America", "Canada"),"NAm",
                                      ifelse(Y_country %in% c("Kenya", "Nigeria", "Russian Federation"),"EMEA", 
                                             ifelse(Y_country %in% c("Pakistan", "Viet Nam"),"AP", 
                                                    ifelse(Y_country %in% c("Venezuela (Bolivarian Republic of)"),"LA", Y_timezone))))) %>%
  filter(!is.na(Y_timezone))

data <-
  data %>% left_join(region, by = "Y_country") 

data$Y_timezone.x <- NULL
data$n <- NULL
data <- data %>% dplyr::rename( Y_timezone = Y_timezone.y)
```

after imputation, missing region data is reduced 93%, (151592 -> 10974)

###jobfamily
**Fill the missing jobfamily with the most frequent value of userid**

```{r, include=FALSE}
data %>% group_by(Z_jobfamily) %>% distinct(Z_jobfamily)
data %>% group_by(Z_position) %>% distinct(Z_position)

jobfamily <- 
  data %>% group_by(Z_cmguserid, Z_jobfamily) %>% dplyr::summarise(n = n()) %>% write_csv("job.csv")

jobfamily <-
  jobfamily %>% group_by(Z_cmguserid) %>% top_n(1, n) %>% filter(!is.na(Z_jobfamily))

data <-
  data %>% left_join(jobfamily, by = "Z_cmguserid") %>%
  mutate(Z_jobfamily.x = ifelse(is.na(Z_jobfamily.x), Z_jobfamily.y, Z_jobfamily.x))
data$Z_jobfamily.y <- NULL
data$n <- NULL
data <- data %>% dplyr::rename( Z_jobfamily = Z_jobfamily.x)

```

**Again let's fill the missing jobfamily with the most frequent value of position**
```{r, include=FALSE}
position <- 
  data %>% group_by(Z_position, Z_jobfamily) %>% dplyr::summarise(n = n())

position <-
  position %>% group_by(Z_position) %>% top_n(1, n) %>% filter(!is.na(Z_jobfamily))

data <-
  data %>% left_join(position, by = "Z_position") %>%
  mutate(Z_jobfamily.x = ifelse(is.na(Z_jobfamily.x), Z_jobfamily.y, Z_jobfamily.x))
data$Z_jobfamily.y <- NULL
data$n <- NULL
data <- data %>% dplyr::rename( Z_jobfamily = Z_jobfamily.x)
```

after imputation, missing jobfamily data is reduced 2%, (119922 -> 118218)

```{r, echo=FALSE}
NAcol <- which(colSums(is.na(data))>0)
NAcols2 <- sort(colSums(sapply(data[NAcol],is.na)), decreasing = T)
NAcols2
```

##Missing value only for internal emplyee 

Assinging missing Country, Region data with the mode of related data is successful, but not significant imputation for job family.
Later incident data will be screened by requestor whose internal employee indicator is "Y" then these missing value will be reduced again.
In this case, total data is 486531, missing data 9559 / thus missing Country, Region, job family is less than 2% for Merck internal requestor.
This is good result after null imputation.

```{r, echo=FALSE}
temp <-
data %>% filter(substr(Z_cmguserid,1,1) == "M")

NAcol <- which(colSums(is.na(temp))>0)
NAcols3 <- sort(colSums(sapply(temp[NAcol],is.na)), decreasing = T)
n3 <- nrow(temp)
NAcols3
```

###BSID
BSID value is important because this is the baseline measurement with business agreement 
**Fill the missing BSID with the most frequent value of assignment group**
```{r, include=FALSE}
#Fill the missing BSID with the most frequent value of assignment_Group
data %>% group_by(X_bs_id) %>% distinct(X_bs_id)
BSID <- 
  data %>% group_by(assignment_grp, X_bs_id) %>% dplyr::summarise(n = n())

BSID <-
  BSID %>% group_by(assignment_grp) %>% top_n(1, n) %>% filter(!is.na(X_bs_id))

data <-
  data %>% left_join(BSID, by = "assignment_grp") %>%
  mutate(X_bs_id.x = ifelse(is.na(X_bs_id.x), X_bs_id.y, X_bs_id.x))
data$X_bs_id.y <- NULL
data$n <- NULL
data <- data %>% dplyr::rename( X_bs_id = X_bs_id.x)
```

* 42,477 out of 83,555 missing BSID is still remained as missing(51%) after null value handling
* with assignment group & service component we can found that most of the missing data is related to SIAL, 34,705 out of 42,777(82%)
* Every momth it occurs around 2500 SIAL incidents which don't have BSID, whereas around 4500 has BSID every month in SIAL ticket
* It needs to identify the causes of missing BSID & improve the situation

```{r, echo=FALSE}
data %>% filter(str_detect(assignment_grp, "SIAL") | str_detect(X_service_component, "SIAL")) %>% filter(!is.na(X_bs_id)) %>% 
  group_by(yyyymm) %>%  dplyr::summarise(n = n())
```

Let's try again with IT engineer who resolve the incident to fill the BSID
38,093 out of 42,477 missing BSID is still remained as missing(89%) after null imputation with IT engineer
Still we have high number of missing BSID, but it is less than 1% of total incidents

```{r, eval=FALSE}
BSID <- 
  data %>% group_by(resolved_by_id, X_bs_id) %>% dplyr::summarise(n = n())

BSID <-
  BSID %>% group_by(resolved_by_id) %>% top_n(1, n) %>% filter(!is.na(resolved_by_id)) %>% filter(!is.na(X_bs_id)) %>% filter(n > 1)

data <-
  data %>% left_join(BSID, by = "resolved_by_id") %>%
  mutate(X_bs_id.x = ifelse(is.na(X_bs_id.x), X_bs_id.y, X_bs_id.x))
data$X_bs_id.y <- NULL
data$n <- NULL
data <- data %>% dplyr::rename( X_bs_id = X_bs_id.x)
```

If it narrow down the incidents created only by the business user not IT, it is 30534 missing values out of total 466,393, 6.5%

```{r, echo=FALSE}
NAcol <- which(colSums(is.na(data))>0)
NAcols5 <- sort(colSums(sapply(data[NAcol],is.na)), decreasing = T)
#NAcols2 #83555 -> 42477 -> 38673 (for internal)

data %>% filter(substr(Z_cmguserid,1,1) == "M", !is.na(resolved), request_div != "IT") %>%  dplyr::summarise(Miss_BSD = sum(is.na(X_bs_id)), Not_Miss_BSD = sum(!is.na(X_bs_id)), 
                                                                                        all_biz_resolved_incidents = Miss_BSD+Not_Miss_BSD, Miss_BSD/(Miss_BSD+Not_Miss_BSD)*100)
```

###Data Review again

Finally let's see characteristics of all the data created by Merck employee including IT

Character data such as "incident_type, request_div, urgency, Y_country, Y_timezone, Z_jobfamily, state, state2" has less than 2% missing data, 
and values(features) in the data are maximum 120. Later these data will be transformed to categorical data

```{r, echo=FALSE}
skimmed %>% filter(type == "character") %>% arrange(type, as.numeric(n_unique), desc(missing))
```

For Numeric data such as LT_day(lead time in day), LT_hour 75th quantile is 2.6 days with 9.5 standard deviation,

```{r, echo=FALSE}
#skimmed %>% filter(type == "numeric") %>% arrange(type, as.numeric(n_unique), desc(missing))
skimmed %>% filter(variable %in% c("LT_day","LT_hour")) %>% select(variable, mean,sd,starts_with("p"))
```

It is true that lead time of most incidents is located within 10 days, but still many data also stay between 15 ~ 45days.

```{r, echo=FALSE}
data %>% filter(substr(Z_cmguserid,1,1) == "M", !is.na(resolved)) %>% 
  ggplot(aes(LT_day)) + geom_histogram(binwidth = 10) + xlim(10,50) 
```

###Reassignment, Reopen

There needs to check the case when Reassignment >= 4, reopen_count >= 2, as it is related to not clear service matirix as well as service lead time